{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02e0d3d-f70d-4f1c-8ced-c6f7e4e66fa7",
      "metadata": {
        "id": "e02e0d3d-f70d-4f1c-8ced-c6f7e4e66fa7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bf3181-71a1-4277-9433-b3e6c0b420fd",
      "metadata": {
        "id": "89bf3181-71a1-4277-9433-b3e6c0b420fd",
        "outputId": "14a03043-3db0-4d8f-d850-947faf8a0006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pickle\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b8c37a-8000-4d43-a5ff-8de831c2bec7",
      "metadata": {
        "id": "c1b8c37a-8000-4d43-a5ff-8de831c2bec7",
        "outputId": "2c3d78d6-49ea-4a85-ad53-29ebed5820bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (5873, 3)\n",
            "\n",
            "Label distribution:\n",
            "label\n",
            "not_applicable    3149\n",
            "hope              1812\n",
            "hate               912\n",
            "Name: count, dtype: int64\n",
            "Label mapping: {'hate': np.int64(0), 'hope': np.int64(1), 'not_applicable': np.int64(2)}\n",
            "Number of classes: 3\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('train.csv')  # Replace with your dataset path\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "num_labels = len(label_encoder.classes_)\n",
        "print(f\"Number of classes: {num_labels}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f1701e-8179-4759-934a-21e07f6014db",
      "metadata": {
        "id": "a8f1701e-8179-4759-934a-21e07f6014db",
        "outputId": "8af9950a-f717-43af-881f-a81dd49ca4a0",
        "colab": {
          "referenced_widgets": [
            "cd9cf95503ac485bb0363caf18432939",
            "737517e33b0342f7923f44621e73ef5e",
            "59b542e1157b4ebaab6bda9fe4978c1c",
            "ef9d74b0bcb84796b91ee061e3b3334e",
            "90cb3618904d486abd62b963ceab4118",
            "b46e0163d269483b8682bb9f62830419"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9cf95503ac485bb0363caf18432939",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "737517e33b0342f7923f44621e73ef5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59b542e1157b4ebaab6bda9fe4978c1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef9d74b0bcb84796b91ee061e3b3334e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90cb3618904d486abd62b963ceab4118",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b46e0163d269483b8682bb9f62830419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#model_name = \"aubmindlab/bert-large-arabertv02\"\n",
        "#CAMeL-Lab/bert-base-arabic-camelbert-da\n",
        "#araelectra-base-discriminator\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af2e391-5b05-450e-b701-8465c380d8e7",
      "metadata": {
        "id": "2af2e391-5b05-450e-b701-8465c380d8e7"
      },
      "outputs": [],
      "source": [
        "class ArabicSentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352d7474-0513-425b-ae03-ac965017e472",
      "metadata": {
        "id": "352d7474-0513-425b-ae03-ac965017e472"
      },
      "outputs": [],
      "source": [
        "train_dataset = ArabicSentimentDataset(\n",
        "    texts=df['text'].tolist(),\n",
        "    labels=df['label_encoded'].tolist(),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6a4919-54b0-4698-a231-38ba10f38f4e",
      "metadata": {
        "id": "8e6a4919-54b0-4698-a231-38ba10f38f4e"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/mnt/c/Users/T2410260/model/Arabert-cache',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_strategy='epoch',\n",
        "\n",
        "    load_best_model_at_end=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=None,\n",
        "    dataloader_pin_memory=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca022a2-3134-4d8a-89ec-a6095f57a323",
      "metadata": {
        "id": "9ca022a2-3134-4d8a-89ec-a6095f57a323",
        "outputId": "d083d058-bfaf-4e13-fa9e-ad7b2dfc085a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1472' max='1472' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1472/1472 01:32, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.817300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.779300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.760400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.755400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.663900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.649500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.632500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.616300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.471200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.464100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1472, training_loss=0.6823376922503762, metrics={'train_runtime': 92.4602, 'train_samples_per_second': 254.077, 'train_steps_per_second': 15.92, 'total_flos': 1545265102316544.0, 'train_loss': 0.6823376922503762, 'epoch': 4.0})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07df1947-744c-4222-b948-66fbd13052fe",
      "metadata": {
        "id": "07df1947-744c-4222-b948-66fbd13052fe",
        "outputId": "45b76f91-a468-470d-e7ea-410418ea3991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model, tokenizer, and label encoder saved successfully!\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained('/mnt/c/Users/T2410260/model/fine_tuned_araelectra-base-discriminator')\n",
        "tokenizer.save_pretrained('/mnt/c/Users/T2410260/model/fine_tuned_araelectra-base-discriminator')\n",
        "\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"Model, tokenizer, and label encoder saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0478d1-8db0-4de0-a446-87e875c92753",
      "metadata": {
        "id": "4a0478d1-8db0-4de0-a446-87e875c92753",
        "outputId": "f53e24c6-a63a-4a06-aa53-aaf913e147f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1476 samples from validation set\n",
            "Making predictions...\n",
            "Processed batch 1/47\n",
            "Processed batch 2/47\n",
            "Processed batch 3/47\n",
            "Processed batch 4/47\n",
            "Processed batch 5/47\n",
            "Processed batch 6/47\n",
            "Processed batch 7/47\n",
            "Processed batch 8/47\n",
            "Processed batch 9/47\n",
            "Processed batch 10/47\n",
            "Processed batch 11/47\n",
            "Processed batch 12/47\n",
            "Processed batch 13/47\n",
            "Processed batch 14/47\n",
            "Processed batch 15/47\n",
            "Processed batch 16/47\n",
            "Processed batch 17/47\n",
            "Processed batch 18/47\n",
            "Processed batch 19/47\n",
            "Processed batch 20/47\n",
            "Processed batch 21/47\n",
            "Processed batch 22/47\n",
            "Processed batch 23/47\n",
            "Processed batch 24/47\n",
            "Processed batch 25/47\n",
            "Processed batch 26/47\n",
            "Processed batch 27/47\n",
            "Processed batch 28/47\n",
            "Processed batch 29/47\n",
            "Processed batch 30/47\n",
            "Processed batch 31/47\n",
            "Processed batch 32/47\n",
            "Processed batch 33/47\n",
            "Processed batch 34/47\n",
            "Processed batch 35/47\n",
            "Processed batch 36/47\n",
            "Processed batch 37/47\n",
            "Processed batch 38/47\n",
            "Processed batch 39/47\n",
            "Processed batch 40/47\n",
            "Processed batch 41/47\n",
            "Processed batch 42/47\n",
            "Processed batch 43/47\n",
            "Processed batch 44/47\n",
            "Processed batch 45/47\n",
            "Processed batch 46/47\n",
            "Processed batch 47/47\n",
            "Predictions saved to submission008.csv\n",
            "Submission zip file created: submission008.zip\n",
            "\n",
            "Sample predictions:\n",
            "     id      prediction\n",
            "0  7434            hope\n",
            "1  5133            hope\n",
            "2  1418  not_applicable\n",
            "3   334  not_applicable\n",
            "4  1600            hope\n",
            "5  9263            hate\n",
            "6  7727            hope\n",
            "7   106            hate\n",
            "8   647            hope\n",
            "9  4715            hate\n",
            "\n",
            "Prediction distribution:\n",
            "prediction\n",
            "not_applicable    885\n",
            "hope              378\n",
            "hate              213\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def predict_validation_set(model, tokenizer, label_encoder, validation_csv_path, output_path='submission.csv', max_length=128):\n",
        "    \"\"\"\n",
        "    Predict sentiments for validation set and create submission file\n",
        "    \"\"\"\n",
        "    # Load validation data\n",
        "    val_df = pd.read_csv(validation_csv_path)\n",
        "    print(f\"Loaded {len(val_df)} samples from validation set\")\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'id' not in val_df.columns or 'text' not in val_df.columns:\n",
        "        raise ValueError(\"Validation CSV must contain 'id' and 'text' columns\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # Process in batches for efficiency\n",
        "    batch_size = 32\n",
        "    total_batches = (len(val_df) + batch_size - 1) // batch_size\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_df), batch_size):\n",
        "            batch_texts = val_df['text'].iloc[i:i+batch_size].tolist()\n",
        "\n",
        "            # Handle NaN or empty texts\n",
        "            batch_texts = [str(text) if pd.notna(text) and text != '' else \"empty text\" for text in batch_texts]\n",
        "\n",
        "            # Tokenize batch\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move to device if using GPU\n",
        "            device = next(model.parameters()).device\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Make predictions\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Get predicted classes\n",
        "            predicted_classes = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
        "\n",
        "            predictions.extend(predicted_labels)\n",
        "\n",
        "            # Progress update\n",
        "            print(f\"Processed batch {(i//batch_size) + 1}/{total_batches}\")\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': val_df['id'],\n",
        "        'prediction': predictions\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    submission_df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "    print(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "    # Create zip file\n",
        "    zip_path = output_path.replace('.csv', '.zip')\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(output_path, os.path.basename(output_path))\n",
        "\n",
        "    print(f\"Submission zip file created: {zip_path}\")\n",
        "\n",
        "    # Display sample predictions\n",
        "    print(\"\\nSample predictions:\")\n",
        "    print(submission_df.head(10))\n",
        "\n",
        "    # Display prediction distribution\n",
        "    print(\"\\nPrediction distribution:\")\n",
        "    print(submission_df['prediction'].value_counts())\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Usage\n",
        "validation_csv_path = 'validation.csv'  # Replace with your validation file path\n",
        "submission_df = predict_validation_set(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    label_encoder=label_encoder,\n",
        "    validation_csv_path=validation_csv_path,\n",
        "    output_path='submission008.csv'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a93a7d8-9e81-474b-99a8-c2bd81223e2b",
      "metadata": {
        "id": "3a93a7d8-9e81-474b-99a8-c2bd81223e2b",
        "outputId": "96335b8f-0944-422e-b8ed-bd8168293021",
        "colab": {
          "referenced_widgets": [
            "f6d3f7f063b847f9ade5bbdf7203d5a2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6d3f7f063b847f9ade5bbdf7203d5a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Login to Hugging Face\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745862f2-b87b-4fe8-bc69-3ef096f9e049",
      "metadata": {
        "id": "745862f2-b87b-4fe8-bc69-3ef096f9e049",
        "outputId": "e46ccf1c-7917-43b5-8902-c76ee6335171",
        "colab": {
          "referenced_widgets": [
            "dcf0469ea35c40898a44779a4caefa18",
            "232d33e467fb4adaafc86c35334e3c0c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcf0469ea35c40898a44779a4caefa18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "232d33e467fb4adaafc86c35334e3c0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer successfully pushed to: https://huggingface.co/Ash2749/araelectra-base-discriminator-t1\n"
          ]
        }
      ],
      "source": [
        "# Define your model repository name\n",
        "repo_name = \"Ash2749/araelectra-base-discriminator-t1\"\n",
        "\n",
        "# Push model with additional options\n",
        "model.push_to_hub(\n",
        "    repo_name,\n",
        "    commit_message=\"Fine-tuned araelectra-base-discriminator BERT for Arabic sentiment analysis\",\n",
        "    private=False  # Set to True if you want a private repository\n",
        ")\n",
        "\n",
        "# Push tokenizer\n",
        "tokenizer.push_to_hub(\n",
        "    repo_name,\n",
        "    commit_message=\"Tokenizer for Arabic sentiment analysis model\"\n",
        ")\n",
        "\n",
        "print(f\"Model and tokenizer successfully pushed to: https://huggingface.co/{repo_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317361dc-7aa6-49ef-9a6d-25d8fc977690",
      "metadata": {
        "id": "317361dc-7aa6-49ef-9a6d-25d8fc977690"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bff5461-40eb-4492-93fd-d99f7919411d",
      "metadata": {
        "id": "4bff5461-40eb-4492-93fd-d99f7919411d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}